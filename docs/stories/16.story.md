# Story 16: Complete Text Extraction Pipeline

## Overview
Implement a comprehensive text extraction system that captures ALL text content from PDFs, not just text within detected zones. This ensures no information is lost during processing and provides users with complete access to their document content.

## User Story
As a user, I want to see all text content from my PDF, so that no information is lost during processing and I have complete access to my document's information.

## Acceptance Criteria
1. Extract all text from PDF pages, including areas outside detected zones
2. Implement fallback extraction for content missed by zone detection
3. Preserve document structure, formatting, and reading order
4. Handle orphaned text blocks and marginalia
5. Maintain spatial relationships between text elements
6. Extract headers, footers, and page numbers correctly
7. Process text that falls between or outside zones
8. Merge zone-based and full-text extraction intelligently

## Tasks / Subtasks

### Task 1: Full-Page Text Extraction Implementation (AC: 1, 3, 5)
**Reference: [Source: architecture.md#text-extraction]**
- Implement PDF.js text layer extraction
- Create page-by-page text extraction
- Preserve text positioning and layout
- Extract all text items with coordinates
- Handle multi-column layouts
- **File Location**: `lib/pdf-processing/full-text-extractor.ts`
- **Unit Tests**: Text extraction completeness, layout preservation

### Task 2: Fallback Extraction System (AC: 2, 7)
**Reference: [Source: architecture.md#fallback-processing]**
- Create fallback extraction pipeline
- Detect gaps in zone coverage
- Extract text from uncovered areas
- Implement coverage analysis
- Add extraction confidence scoring
- **File Location**: `lib/pdf-processing/fallback-extractor.ts`
- **Unit Tests**: Gap detection, fallback accuracy

### Task 3: Text Structure Preservation (AC: 3, 5)
**Reference: [Source: architecture.md#document-structure]**
- Implement reading order detection
- Preserve paragraph boundaries
- Maintain list structures
- Handle table text extraction
- Keep heading hierarchies
- **File Location**: `lib/pdf-processing/structure-analyzer.ts`
- **Unit Tests**: Structure detection, order preservation

### Task 4: Marginalia and Orphaned Text Handler (AC: 4, 6)
**Reference: [Source: architecture.md#edge-cases]**
- Detect headers and footers
- Extract page numbers
- Handle margin notes
- Process floating text blocks
- Identify captions and labels
- **File Location**: `lib/pdf-processing/marginalia-handler.ts`
- **Unit Tests**: Edge case detection, classification

### Task 5: Content Reconciliation Engine (AC: 8)
**Reference: [Source: architecture.md#content-merging]**
- Merge zone and full-text extractions
- Resolve overlapping content
- Handle duplicate detection
- Maintain quality scoring
- Create unified content model
- **File Location**: `lib/pdf-processing/content-reconciler.ts`
- **Unit Tests**: Merge accuracy, conflict resolution

### Task 6: Enhanced Processing Pipeline Integration (AC: 1, 2, 8)
**Reference: [Source: architecture.md#pipeline-integration]**
- Update orchestrator for dual extraction
- Modify processing workflow
- Add extraction strategy selection
- Implement progress tracking
- Update status reporting
- **File Location**: `lib/pdf-processing/enhanced-pipeline.ts`
- **Unit Tests**: Pipeline flow, integration

### Task 7: Content Display Updates (AC: 1, 3)
**Reference: [Source: architecture.md#ui-updates]**
- Update ExtractedContentViewer component
- Display non-zone content
- Show extraction source indicators
- Add content type labels
- Implement view toggling
- **File Location**: `app/components/viewer/ExtractedContentViewer.tsx`
- **Unit Tests**: Display accuracy, UI updates

### Task 8: Performance Optimization (AC: 1, 8)
**Reference: [Source: architecture.md#performance]**
- Implement extraction caching
- Add parallel processing
- Optimize memory usage
- Create progressive loading
- Add extraction cancellation
- **File Location**: `lib/pdf-processing/extraction-optimizer.ts`
- **Unit Tests**: Performance benchmarks, resource usage

## Dev Notes

### Full-Page Text Extraction [Source: architecture.md#pdf-text-extraction]
```typescript
import { PDFDocumentProxy, PDFPageProxy } from 'pdfjs-dist';

interface ExtractedText {
  text: string;
  x: number;
  y: number;
  width: number;
  height: number;
  fontSize: number;
  fontName: string;
}

class FullTextExtractor {
  async extractAllText(pdfDoc: PDFDocumentProxy): Promise<PageText[]> {
    const pageTexts: PageText[] = [];
    
    for (let pageNum = 1; pageNum <= pdfDoc.numPages; pageNum++) {
      const page = await pdfDoc.getPage(pageNum);
      const textContent = await page.getTextContent();
      const viewport = page.getViewport({ scale: 1.0 });
      
      const extractedItems: ExtractedText[] = textContent.items.map(item => {
        const transform = item.transform;
        const x = transform[4];
        const y = viewport.height - transform[5];
        
        return {
          text: item.str,
          x,
          y,
          width: item.width,
          height: item.height,
          fontSize: Math.sqrt(transform[0] * transform[0] + transform[1] * transform[1]),
          fontName: item.fontName || 'unknown'
        };
      });
      
      pageTexts.push({
        pageNumber: pageNum,
        items: extractedItems,
        viewport: {
          width: viewport.width,
          height: viewport.height
        }
      });
    }
    
    return pageTexts;
  }
  
  private detectTextBlocks(items: ExtractedText[]): TextBlock[] {
    // Group text items into logical blocks
    const blocks: TextBlock[] = [];
    const sorted = [...items].sort((a, b) => a.y - b.y || a.x - b.x);
    
    let currentBlock: ExtractedText[] = [];
    let lastItem: ExtractedText | null = null;
    
    for (const item of sorted) {
      if (lastItem && this.shouldStartNewBlock(lastItem, item)) {
        if (currentBlock.length > 0) {
          blocks.push(this.createTextBlock(currentBlock));
        }
        currentBlock = [item];
      } else {
        currentBlock.push(item);
      }
      lastItem = item;
    }
    
    if (currentBlock.length > 0) {
      blocks.push(this.createTextBlock(currentBlock));
    }
    
    return blocks;
  }
  
  private shouldStartNewBlock(prev: ExtractedText, curr: ExtractedText): boolean {
    const lineHeight = prev.height * 1.5;
    const verticalGap = Math.abs(curr.y - prev.y);
    const horizontalGap = curr.x - (prev.x + prev.width);
    
    return verticalGap > lineHeight || horizontalGap > prev.width * 2;
  }
}
```

### Fallback Extraction System [Source: architecture.md#fallback-extraction]
```typescript
interface CoverageMap {
  covered: Rectangle[];
  uncovered: Rectangle[];
  coverage: number;
}

class FallbackExtractor {
  analyzeCoverage(pageSize: Size, zones: Zone[]): CoverageMap {
    const pageArea = pageSize.width * pageSize.height;
    const coveredRects: Rectangle[] = zones.map(z => z.coordinates);
    
    // Create coverage grid
    const grid = this.createCoverageGrid(pageSize, 10); // 10x10 grid
    const uncoveredCells = [];
    
    for (const cell of grid) {
      if (!this.isCovered(cell, coveredRects)) {
        uncoveredCells.push(cell);
      }
    }
    
    // Merge adjacent uncovered cells
    const uncoveredRects = this.mergeRectangles(uncoveredCells);
    const uncoveredArea = uncoveredRects.reduce((sum, r) => sum + r.width * r.height, 0);
    
    return {
      covered: coveredRects,
      uncovered: uncoveredRects,
      coverage: (pageArea - uncoveredArea) / pageArea
    };
  }
  
  async extractFromGaps(
    page: PDFPageProxy,
    uncoveredAreas: Rectangle[]
  ): Promise<ExtractedContent[]> {
    const textContent = await page.getTextContent();
    const gapContent: ExtractedContent[] = [];
    
    for (const area of uncoveredAreas) {
      const itemsInArea = textContent.items.filter(item => {
        const itemRect = this.getItemRectangle(item);
        return this.intersects(itemRect, area);
      });
      
      if (itemsInArea.length > 0) {
        gapContent.push({
          area,
          text: itemsInArea.map(i => i.str).join(' '),
          confidence: 0.8, // Lower confidence for fallback
          source: 'fallback',
          items: itemsInArea
        });
      }
    }
    
    return gapContent;
  }
}
```

### Content Reconciliation [Source: architecture.md#content-reconciliation]
```typescript
interface UnifiedContent {
  zones: EnhancedZone[];
  additionalContent: ExtractedContent[];
  fullText: string;
  structure: DocumentStructure;
}

class ContentReconciler {
  reconcile(
    zones: Zone[],
    fullTextExtraction: PageText[],
    fallbackContent: ExtractedContent[]
  ): UnifiedContent {
    const enhancedZones: EnhancedZone[] = [];
    const additionalContent: ExtractedContent[] = [];
    
    // Enhance zones with full text data
    for (const zone of zones) {
      const zoneText = this.extractTextForZone(zone, fullTextExtraction);
      const enhanced: EnhancedZone = {
        ...zone,
        fullText: zoneText.text,
        textItems: zoneText.items,
        hasCompleteText: zoneText.coverage > 0.95
      };
      enhancedZones.push(enhanced);
    }
    
    // Add fallback content that doesn't overlap with zones
    for (const content of fallbackContent) {
      if (!this.overlapsWithZones(content.area, zones)) {
        additionalContent.push(content);
      }
    }
    
    // Build document structure
    const structure = this.buildDocumentStructure(
      enhancedZones,
      additionalContent,
      fullTextExtraction
    );
    
    return {
      zones: enhancedZones,
      additionalContent,
      fullText: this.assembleFullText(structure),
      structure
    };
  }
  
  private buildDocumentStructure(
    zones: EnhancedZone[],
    additional: ExtractedContent[],
    fullText: PageText[]
  ): DocumentStructure {
    const elements: DocumentElement[] = [];
    
    // Analyze text patterns for structure
    for (const pageText of fullText) {
      const headings = this.detectHeadings(pageText.items);
      const paragraphs = this.detectParagraphs(pageText.items);
      const lists = this.detectLists(pageText.items);
      
      elements.push(...headings, ...paragraphs, ...lists);
    }
    
    // Sort by reading order
    elements.sort((a, b) => {
      if (a.page !== b.page) return a.page - b.page;
      return a.y - b.y || a.x - b.x;
    });
    
    return {
      elements,
      hierarchy: this.buildHierarchy(elements),
      readingOrder: elements.map(e => e.id)
    };
  }
}
```

### Reading Order Detection [Source: architecture.md#reading-order]
```typescript
class ReadingOrderAnalyzer {
  detectReadingOrder(items: ExtractedText[]): string[] {
    // Detect columns
    const columns = this.detectColumns(items);
    
    if (columns.length > 1) {
      return this.orderMultiColumn(columns);
    } else {
      return this.orderSingleColumn(items);
    }
  }
  
  private detectColumns(items: ExtractedText[]): Column[] {
    const xPositions = items.map(i => i.x).sort((a, b) => a - b);
    const gaps = [];
    
    // Find large horizontal gaps
    for (let i = 1; i < xPositions.length; i++) {
      const gap = xPositions[i] - xPositions[i - 1];
      if (gap > 50) { // Threshold for column gap
        gaps.push((xPositions[i - 1] + xPositions[i]) / 2);
      }
    }
    
    // Group items by columns
    const columns: Column[] = [];
    const boundaries = [0, ...gaps, Infinity];
    
    for (let i = 0; i < boundaries.length - 1; i++) {
      const columnItems = items.filter(
        item => item.x >= boundaries[i] && item.x < boundaries[i + 1]
      );
      
      if (columnItems.length > 0) {
        columns.push({
          left: boundaries[i],
          right: boundaries[i + 1],
          items: columnItems
        });
      }
    }
    
    return columns;
  }
}
```

### Performance Optimization [Source: architecture.md#extraction-performance]
```typescript
class ExtractionOptimizer {
  private cache = new Map<string, ExtractionResult>();
  private workers: Worker[] = [];
  
  async optimizedExtraction(
    pdfDoc: PDFDocumentProxy,
    options: ExtractionOptions
  ): Promise<ExtractionResult> {
    const cacheKey = this.getCacheKey(pdfDoc, options);
    
    // Check cache
    if (this.cache.has(cacheKey)) {
      return this.cache.get(cacheKey)!;
    }
    
    // Parallel extraction
    const numWorkers = Math.min(4, pdfDoc.numPages);
    const pagesPerWorker = Math.ceil(pdfDoc.numPages / numWorkers);
    
    const promises: Promise<PageExtractionResult>[] = [];
    
    for (let i = 0; i < numWorkers; i++) {
      const startPage = i * pagesPerWorker + 1;
      const endPage = Math.min((i + 1) * pagesPerWorker, pdfDoc.numPages);
      
      promises.push(
        this.extractPagesInWorker(pdfDoc, startPage, endPage, options)
      );
    }
    
    const results = await Promise.all(promises);
    const combined = this.combineResults(results);
    
    // Cache result
    this.cache.set(cacheKey, combined);
    
    return combined;
  }
  
  private async extractPagesInWorker(
    pdfDoc: PDFDocumentProxy,
    startPage: number,
    endPage: number,
    options: ExtractionOptions
  ): Promise<PageExtractionResult> {
    return new Promise((resolve, reject) => {
      const worker = new Worker('/workers/text-extractor.js');
      
      worker.onmessage = (e) => {
        if (e.data.type === 'complete') {
          resolve(e.data.result);
          worker.terminate();
        } else if (e.data.type === 'error') {
          reject(new Error(e.data.error));
          worker.terminate();
        }
      };
      
      worker.postMessage({
        command: 'extract',
        pdfData: pdfDoc,
        startPage,
        endPage,
        options
      });
    });
  }
}
```

### File Locations [Source: architecture.md#repository-structure]
- **Full Text Extractor**: `lib/pdf-processing/full-text-extractor.ts`
- **Fallback Extractor**: `lib/pdf-processing/fallback-extractor.ts`
- **Structure Analyzer**: `lib/pdf-processing/structure-analyzer.ts`
- **Marginalia Handler**: `lib/pdf-processing/marginalia-handler.ts`
- **Content Reconciler**: `lib/pdf-processing/content-reconciler.ts`
- **Enhanced Pipeline**: `lib/pdf-processing/enhanced-pipeline.ts`
- **Extraction Optimizer**: `lib/pdf-processing/extraction-optimizer.ts`
- **Updated Viewer**: `app/components/viewer/ExtractedContentViewer.tsx`

### Testing Requirements [Source: architecture.md#testing]
- **Unit Tests**: Text extraction accuracy, structure preservation
- **Integration Tests**: Pipeline flow, content reconciliation
- **Performance Tests**: Large document handling, memory usage
- **Edge Case Tests**: Complex layouts, special characters
- **Visual Tests**: UI display accuracy

### Performance Targets [Source: architecture.md#performance]
- **Extraction Speed**: <2s for 10-page document
- **Memory Usage**: <200MB for 100-page document
- **Cache Hit Rate**: >90% for repeated extractions
- **Accuracy**: >99% text capture rate
- **Structure Detection**: >95% accuracy

## Project Structure Notes
This story enhances the existing PDF processing pipeline to ensure complete text extraction. It works alongside the zone-based extraction system, providing a comprehensive fallback mechanism that guarantees no content is lost during processing.

## Dependencies
- ✅ PDF.js integration
- ✅ Existing zone detection system
- ✅ Processing orchestrator
- ✅ ExtractedContentViewer component
- ⏳ Enhanced processing pipeline

## Status
Ready for Implementation

## Estimated Effort
- **Full-Page Extraction**: 2 days
- **Fallback System**: 1.5 days
- **Structure Preservation**: 2 days
- **Marginalia Handler**: 1 day
- **Content Reconciliation**: 2 days
- **Pipeline Integration**: 1.5 days
- **UI Updates**: 1 day
- **Performance Optimization**: 1 day
- **Testing**: 2 days
**Total**: 13 days

## Definition of Done
- [ ] All PDF text is extracted, including non-zone areas
- [ ] Fallback extraction catches missed content
- [ ] Document structure is preserved
- [ ] Reading order is maintained
- [ ] Headers, footers, and margins are handled
- [ ] Content reconciliation works accurately
- [ ] UI displays all extracted content
- [ ] Performance targets are met
- [ ] All tests pass with >95% coverage
- [ ] Documentation is updated
- [ ] Code review completed

---
*Story 16 - Epic 5: Missing Core Features*