# Story 13: Database Integration and Schema

## Overview
Implement comprehensive database integration using Supabase/PostgreSQL to persist all platform data including documents, zones, processing history, and export records. This story establishes the data persistence layer that enables users to manage their documents and maintain processing state across sessions.

## User Story
As a system, I want to persist all platform data, so that users can access their documents and processing history reliably across sessions.

## Acceptance Criteria
1. Supabase project created and configured with proper settings
2. PostgreSQL schema designed and deployed for all entities
3. Document and zone metadata storage with relationships
4. Processing history tracking with detailed status
5. Export records management with audit trail
6. User session data storage for preferences
7. Database migrations system for schema evolution
8. Connection pooling and query optimization

## Tasks / Subtasks

### Task 1: Supabase Project Setup (AC: 1)
**Reference: [Source: architecture.md#database-configuration]**
- Create Supabase project with appropriate tier
- Configure database settings and regions
- Set up environment variables
- Create database roles and permissions
- Configure connection pooling
- **File Location**: `supabase/config.toml`
- **Unit Tests**: Connection testing, configuration validation

### Task 2: Core Database Schema (AC: 2, 3)
**Reference: [Source: architecture.md#database-schema]**
- Design normalized schema for all entities
- Create documents table with metadata
- Create zones table with relationships
- Create processing_jobs table
- Create export_records table
- **File Location**: `supabase/migrations/001_initial_schema.sql`
- **Unit Tests**: Schema validation, constraint testing

### Task 3: Document Storage Integration (AC: 3)
**Reference: [Source: architecture.md#document-management]**
- Implement document metadata storage
- Create storage buckets for PDFs
- Set up file upload policies
- Implement document versioning
- Add document sharing capabilities
- **File Location**: `backend/app/repositories/document_repository.py`
- **Unit Tests**: CRUD operations, file handling

### Task 4: Zone Management Schema (AC: 3, 4)
**Reference: [Source: architecture.md#zone-management]**
- Create zone metadata tables
- Implement zone-document relationships
- Add zone processing status tracking
- Create zone content storage
- Implement zone history tracking
- **File Location**: `backend/app/repositories/zone_repository.py`
- **Unit Tests**: Zone operations, relationship integrity

### Task 5: Processing State Management (AC: 4)
**Reference: [Source: architecture.md#processing-state]**
- Create processing job tables
- Implement status transition tracking
- Add processing metrics storage
- Create error logging tables
- Implement retry history
- **File Location**: `backend/app/repositories/processing_repository.py`
- **Unit Tests**: State transitions, metrics tracking

### Task 6: Export Records System (AC: 5)
**Reference: [Source: architecture.md#export-management]**
- Create export records schema
- Implement export history tracking
- Add export file references
- Create validation results storage
- Implement partial export tracking
- **File Location**: `backend/app/repositories/export_repository.py`
- **Unit Tests**: Export tracking, file management

### Task 7: Database Migration System (AC: 7)
**Reference: [Source: architecture.md#migrations]**
- Set up Alembic for migrations
- Create initial migration scripts
- Implement rollback procedures
- Add migration validation
- Create migration documentation
- **File Location**: `backend/alembic/`
- **Unit Tests**: Migration execution, rollback testing

### Task 8: Query Optimization (AC: 8)
**Reference: [Source: architecture.md#performance-optimization]**
- Create database indexes
- Implement query optimization
- Add connection pooling
- Create materialized views
- Implement caching strategies
- **File Location**: `supabase/migrations/002_indexes_optimization.sql`
- **Unit Tests**: Query performance, load testing

## Dev Notes

### Database Schema Design [Source: architecture.md#database-schema]
```sql
-- Documents table
CREATE TABLE documents (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    filename VARCHAR(255) NOT NULL,
    original_filename VARCHAR(255) NOT NULL,
    file_size_bytes BIGINT NOT NULL,
    page_count INTEGER,
    mime_type VARCHAR(100),
    storage_path VARCHAR(500),
    status VARCHAR(50) DEFAULT 'uploaded',
    uploaded_by VARCHAR(255),
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    metadata JSONB DEFAULT '{}'::jsonb
);

-- Zones table
CREATE TABLE zones (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID NOT NULL REFERENCES documents(id) ON DELETE CASCADE,
    zone_index INTEGER NOT NULL,
    page_number INTEGER NOT NULL,
    zone_type VARCHAR(50) NOT NULL,
    coordinates JSONB NOT NULL,
    content TEXT,
    confidence DECIMAL(3,2),
    processing_tool VARCHAR(100),
    status VARCHAR(50) DEFAULT 'pending',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    metadata JSONB DEFAULT '{}'::jsonb,
    UNIQUE(document_id, zone_index)
);

-- Processing jobs table
CREATE TABLE processing_jobs (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID NOT NULL REFERENCES documents(id) ON DELETE CASCADE,
    status VARCHAR(50) NOT NULL DEFAULT 'queued',
    strategy VARCHAR(50),
    started_at TIMESTAMPTZ,
    completed_at TIMESTAMPTZ,
    progress DECIMAL(5,2) DEFAULT 0,
    current_zone_id UUID REFERENCES zones(id),
    total_zones INTEGER,
    completed_zones INTEGER DEFAULT 0,
    error_count INTEGER DEFAULT 0,
    options JSONB DEFAULT '{}'::jsonb,
    metrics JSONB DEFAULT '{}'::jsonb,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Export records table
CREATE TABLE export_records (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID NOT NULL REFERENCES documents(id) ON DELETE CASCADE,
    export_type VARCHAR(50) NOT NULL,
    formats TEXT[] NOT NULL,
    status VARCHAR(50) DEFAULT 'pending',
    file_paths JSONB,
    validation_results JSONB,
    selection JSONB,
    started_at TIMESTAMPTZ,
    completed_at TIMESTAMPTZ,
    file_size_bytes BIGINT,
    created_by VARCHAR(255),
    created_at TIMESTAMPTZ DEFAULT NOW(),
    metadata JSONB DEFAULT '{}'::jsonb
);

-- Processing history table
CREATE TABLE processing_history (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    job_id UUID NOT NULL REFERENCES processing_jobs(id),
    zone_id UUID REFERENCES zones(id),
    action VARCHAR(100) NOT NULL,
    status VARCHAR(50),
    details JSONB,
    error_message TEXT,
    duration_ms INTEGER,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- User corrections table
CREATE TABLE user_corrections (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    zone_id UUID NOT NULL REFERENCES zones(id),
    original_content TEXT,
    corrected_content TEXT NOT NULL,
    confidence_before DECIMAL(3,2),
    confidence_after DECIMAL(3,2) DEFAULT 1.0,
    correction_type VARCHAR(50),
    corrected_by VARCHAR(255),
    created_at TIMESTAMPTZ DEFAULT NOW()
);
```

### Supabase Client Integration [Source: architecture.md#database-client]
```python
from supabase import create_client, Client
from typing import Optional, List, Dict
import os

class SupabaseClient:
    def __init__(self):
        url = os.environ.get("SUPABASE_URL")
        key = os.environ.get("SUPABASE_ANON_KEY")
        self.client: Client = create_client(url, key)
    
    async def get_document(self, document_id: str) -> Optional[Dict]:
        response = self.client.table("documents") \
            .select("*") \
            .eq("id", document_id) \
            .single() \
            .execute()
        return response.data
    
    async def create_document(self, document_data: Dict) -> Dict:
        response = self.client.table("documents") \
            .insert(document_data) \
            .execute()
        return response.data[0]
    
    async def get_zones_for_document(self, document_id: str) -> List[Dict]:
        response = self.client.table("zones") \
            .select("*") \
            .eq("document_id", document_id) \
            .order("zone_index") \
            .execute()
        return response.data
```

### Repository Pattern Implementation [Source: architecture.md#repository-pattern]
```python
from typing import Optional, List, Dict
from datetime import datetime
import asyncpg

class DocumentRepository:
    def __init__(self, db_pool: asyncpg.Pool):
        self.db_pool = db_pool
    
    async def create(self, document_data: Dict) -> Dict:
        async with self.db_pool.acquire() as conn:
            row = await conn.fetchrow("""
                INSERT INTO documents (filename, original_filename, file_size_bytes, 
                                     page_count, mime_type, storage_path, uploaded_by)
                VALUES ($1, $2, $3, $4, $5, $6, $7)
                RETURNING *
            """, document_data['filename'], document_data['original_filename'],
                document_data['file_size_bytes'], document_data['page_count'],
                document_data['mime_type'], document_data['storage_path'],
                document_data['uploaded_by'])
            return dict(row)
    
    async def get_by_id(self, document_id: str) -> Optional[Dict]:
        async with self.db_pool.acquire() as conn:
            row = await conn.fetchrow("""
                SELECT * FROM documents WHERE id = $1
            """, document_id)
            return dict(row) if row else None
    
    async def update_status(self, document_id: str, status: str) -> bool:
        async with self.db_pool.acquire() as conn:
            result = await conn.execute("""
                UPDATE documents 
                SET status = $2, updated_at = NOW()
                WHERE id = $1
            """, document_id, status)
            return result != "UPDATE 0"
```

### Storage Bucket Configuration [Source: architecture.md#storage]
```python
class StorageManager:
    def __init__(self, supabase_client: Client):
        self.client = supabase_client
        self.bucket_name = "documents"
    
    async def upload_document(self, file_path: str, file_data: bytes) -> str:
        # Upload to Supabase storage
        response = self.client.storage.from_(self.bucket_name) \
            .upload(file_path, file_data)
        
        if response.error:
            raise Exception(f"Upload failed: {response.error}")
        
        # Get public URL
        url = self.client.storage.from_(self.bucket_name) \
            .get_public_url(file_path)
        
        return url
    
    async def delete_document(self, file_path: str) -> bool:
        response = self.client.storage.from_(self.bucket_name) \
            .remove([file_path])
        return response.error is None
```

### Database Indexes [Source: architecture.md#performance-optimization]
```sql
-- Document indexes
CREATE INDEX idx_documents_status ON documents(status);
CREATE INDEX idx_documents_created_at ON documents(created_at DESC);
CREATE INDEX idx_documents_uploaded_by ON documents(uploaded_by);

-- Zone indexes
CREATE INDEX idx_zones_document_id ON zones(document_id);
CREATE INDEX idx_zones_status ON zones(status);
CREATE INDEX idx_zones_confidence ON zones(confidence);
CREATE INDEX idx_zones_page_number ON zones(document_id, page_number);

-- Processing job indexes
CREATE INDEX idx_processing_jobs_document_id ON processing_jobs(document_id);
CREATE INDEX idx_processing_jobs_status ON processing_jobs(status);
CREATE INDEX idx_processing_jobs_created_at ON processing_jobs(created_at DESC);

-- Export record indexes
CREATE INDEX idx_export_records_document_id ON export_records(document_id);
CREATE INDEX idx_export_records_status ON export_records(status);
CREATE INDEX idx_export_records_created_at ON export_records(created_at DESC);
```

### Migration Example [Source: architecture.md#migrations]
```python
"""Add zone confidence history

Revision ID: 002
Revises: 001
Create Date: 2024-01-15 10:00:00.000000

"""
from alembic import op
import sqlalchemy as sa

def upgrade():
    op.create_table(
        'zone_confidence_history',
        sa.Column('id', sa.UUID(), primary_key=True),
        sa.Column('zone_id', sa.UUID(), sa.ForeignKey('zones.id'), nullable=False),
        sa.Column('confidence', sa.Numeric(3, 2), nullable=False),
        sa.Column('processing_tool', sa.String(100)),
        sa.Column('recorded_at', sa.DateTime(timezone=True), server_default=sa.func.now()),
    )
    
    op.create_index('idx_zone_confidence_history_zone_id', 
                    'zone_confidence_history', ['zone_id'])

def downgrade():
    op.drop_index('idx_zone_confidence_history_zone_id')
    op.drop_table('zone_confidence_history')
```

### Connection Pool Configuration [Source: architecture.md#database-configuration]
```python
import asyncpg
from contextlib import asynccontextmanager

class DatabasePool:
    def __init__(self, database_url: str):
        self.database_url = database_url
        self.pool = None
    
    async def init_pool(self):
        self.pool = await asyncpg.create_pool(
            self.database_url,
            min_size=10,
            max_size=20,
            max_queries=50000,
            max_inactive_connection_lifetime=300,
            command_timeout=60
        )
    
    async def close_pool(self):
        if self.pool:
            await self.pool.close()
    
    @asynccontextmanager
    async def acquire(self):
        async with self.pool.acquire() as connection:
            yield connection
```

### File Locations [Source: architecture.md#repository-structure]
- **Supabase Config**: `supabase/config.toml`
- **Migrations**: `supabase/migrations/`
- **Repositories**: `backend/app/repositories/`
- **Database Client**: `backend/app/database/client.py`
- **Storage Manager**: `backend/app/storage/manager.py`
- **Alembic Config**: `backend/alembic.ini`
- **Migration Scripts**: `backend/alembic/versions/`

### Testing Requirements [Source: architecture.md#testing]
- **Unit Tests**: Repository CRUD operations
- **Integration Tests**: Database transactions
- **Migration Tests**: Schema evolution
- **Performance Tests**: Query optimization
- **Load Tests**: Connection pool limits

### Performance Targets [Source: architecture.md#performance]
- **Query Response**: <50ms for indexed queries
- **Bulk Operations**: 1000+ records/second
- **Connection Pool**: Support 100+ concurrent connections
- **Transaction Time**: <100ms for typical operations
- **Index Performance**: 10x improvement on searches

## Project Structure Notes
The database layer provides persistent storage for all platform data using Supabase's PostgreSQL database. The repository pattern ensures clean separation between business logic and data access. Connection pooling and query optimization ensure high performance at scale.

## Dependencies
- ✅ PostgreSQL 15+
- ✅ Supabase account and project
- ✅ Python asyncpg library
- ✅ Alembic for migrations
- ✅ FastAPI backend (Story 12)
- ⏳ Export system integration

## Status
Ready for Implementation

## Estimated Effort
- **Supabase Setup**: 0.5 days
- **Schema Design**: 1 day
- **Document Storage**: 1 day
- **Zone Management**: 1 day
- **Processing State**: 1 day
- **Export Records**: 0.5 days
- **Migration System**: 1 day
- **Optimization**: 1 day
- **Testing**: 1 day
**Total**: 8 days

## Definition of Done
- [ ] Supabase project created and configured
- [ ] All database tables created with proper schema
- [ ] Indexes implemented for performance
- [ ] Repository pattern implemented for all entities
- [ ] Storage buckets configured for files
- [ ] Migration system operational
- [ ] Connection pooling configured
- [ ] All CRUD operations tested
- [ ] Performance benchmarks met
- [ ] Integration with FastAPI complete
- [ ] Documentation updated

---
*Story 13 - Epic 4: Backend Infrastructure and Integration*